# Создание и обучение компактной языковой модели GPT-архитектуры с нуля на русском языке

## Описание Проекта

Данный проект посвящен исследованию и практической реализации полного цикла создания и обучения компактной языковой модели (Large Language Model - LLM) с нуля. Проект основан на архитектуре Transformer (аналогичной GPT) и использует минималистичную реализацию `nanoGPT`. Основная цель работы заключалась в изучении возможности обучения такой модели на самостоятельно подготовленном русскоязычном корпусе в условиях ограниченных вычислительных ресурсов.

Работа охватывает все ключевые этапы: от сбора и многоступенчатой предобработки данных из различных источников, включая генерацию синтетического корпуса, до адаптации кодовой базы для работы с выбранным токенизатором, проведения процесса обучения модели с нуля и последующего анализа ее возможностей и ограничений.

Проект демонстрирует вызовы и решения, связанные с подготовкой масштабных датасетов для обучения LLM, интеграцией современных токенизаторов и тренировкой моделей на потребительском оборудовании.

## Структура Репозитория

Репозиторий организован в несколько основных папок, каждая из которых содержит скрипты и ресурсы, относящиеся к определенному этапу пайплайна подготовки данных и обучения модели:

- `1_myGPTdistr/`:
  Содержит скрипты для **генерации синтетических статей с нуля на основе заданных тем** с использованием более крупной языковой модели. Этот пайплайн предназначен для создания нового текстового контента для расширения обучающего корпуса.

- `2_myGPTWiki/`:
  Включает скрипты для **обработки статей из исходного корпуса `google/wiki40b` (русскоязычная часть)**. Здесь выполняются задачи по очистке текста от разметки и "шума", фильтрации статей по длине и другим критериям, а также **генерации синтетических статей на основе уже существующих статей Wiki** с помощью внешней языковой модели.

- `3_mDeBERTa_classifier/`:
  Содержит код для **классификации статей из корпуса Wiki по тематическим категориям** с использованием предобученной модели mDeBERTa. Результаты классификации используются на этапе отбора статей для формирования сбалансированной выборки.

- `4_myGPT_train/`:
  Является основной папкой, объединяющей результаты работы предыдущих этапов. Здесь находятся скрипты для **подготовки финального обучающего датасета** путем агрегации обработанных и сгенерированных данных, их **токенизации** с использованием выбранного токенизатора (Gemma), разделения на обучающую и валидационную выборки и сохранения в бинарном формате (`train.bin`, `val.bin`), необходимом для `nanoGPT`. Эта папка также содержит **модифицированную кодовую базу `nanoGPT`** для проведения процесса обучения модели с нуля.

## Ключевые Особенности

* **Комплексный пайплайн подготовки данных:** Разработка и применение многоступенчатого процесса для создания масштабного и чистого обучающего корпуса из разнородных источников.
* **Генерация синтетических данных:** Использование внешних LLM для расширения и улучшения качества обучающего корпуса.
* **Адаптация кодовой базы:** Модификация `nanoGPT` для совместимости с токенизатором с большим размером словаря.
* **Обучение модели с нуля:** Тренировка компактной модели GPT-архитектуры на самостоятельно подготовленных данных без использования предобученных весов.
* **Анализ результатов и ограничений:** Детальное изучение поведения обученной модели, выявление ее возможностей и характерных артефактов генерации, таких как повторения и галлюцинации, и их научное обоснование.

## Установка и Использование (Общий Обзор)

Проект требует установки Python и ряда библиотек для работы с текстом, нейронными сетями и данными (например, `transformers`, `torch`, `numpy`, `tqdm`).

Общий рабочий процесс включает выполнение скриптов в следующей последовательности:

1.  Скрипты в `2_myGPTWiki/` для обработки Wiki.
2.  Скрипты в `3_mDeBERTa_classifier/` для классификации (требует результатов из 2).
3.  Скрипты в `1_myGPTdistr/` для генерации с нуля.
4.  Скрипты подготовки данных в `4_myGPT_train/` (требуют результатов из 1, 2, 3).
5.  Скрипт обучения `train.py` в `4_myGPT_train/nanoGPT/` (требует подготовленных бинарных данных).
6.  Скрипт для запуска модели и генерации текстов `mysample.py` в `4_myGPT_train/nanoGPT/` (требует готовую обученную модель)

Конкретные инструкции по запуску каждого этапа можно найти в соответствующих папках или в файле отчета по проекту.

## Результаты и Анализ

В результате проекта была успешно обучена базовая автодополняющая языковая модель. Анализ показал, что модель усвоила локальную грамматику, структуру текстов и способна воспроизводить форматы обучающих данных. Однако, из-за ограничений, связанных с архитектурой (распределение параметров), объемом данных (низкое соотношение данных/параметров) и временем обучения, модель демонстрирует устойчивые проблемы с долгосрочной семантической связностью, повторениями и галлюцинациями. Детальный анализ результатов и их причин представлен в отчете по проекту.

## Дальнейшее Развитие

Возможные направления развития проекта включают:
* Увеличение объема и качества обучающих данных.
* Эксперименты с более крупными или иными конфигурациями архитектуры модели.
* Более длительное обучение для достижения полной сходимости.
* Исследование методов тонкой настройки (Instruct-тюнинг) после получения более сильной базовой модели.

## Благодарности и Использованные Ресурсы

Данный проект в значительной степени опирается на открытые ресурсы и кодовые базы. Особая благодарность выражается **Андрею Карпатому** (Andrej Karpathy) за создание и публикацию минималистичной и высококачественной реализации архитектуры GPT — проекта **`nanoGPT`**.

Кодовая база `nanoGPT` послужила основой для реализации процесса обучения модели в данном проекте, будучи адаптированной для работы с выбранным токенизатором и спецификой подготовки данных.

Оригинальный репозиторий `nanoGPT` доступен по ссылке:
[https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)

Адаптированная кодовая база `nanoGPT` в рамках данного проекта расположена в директории `4_myGPT_train/nanoGPT/`.

## Контакты

Никита Шильников
shilnikovnikita63@gmail.com
